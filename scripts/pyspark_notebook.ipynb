{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone project\n",
    "\n",
    "This is a near identical copy of the main capstone project in the form of a PySpark notebook on AWS EMR. Please note that the reason this copy is \"near identical\" is due to the lack of an argument parser. One could input their desired input and output path below this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af4361c1d78483d86bae8117ddaa7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1586614114215_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-21-117.us-west-1.compute.internal:20888/proxy/application_1586614114215_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-18-130.us-west-1.compute.internal:8042/node/containerlogs/container_1586614114215_0008_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_path = 's3a://date-engineering-capstone/data/'\n",
    "output_path = 's3://date-engineering-capstone/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03f4b37cf68401b93ed0c4cd092c302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "from typing import Dict, Tuple, List, Optional, Any\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a8236688184079a0ce803c700d8380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_spark_session(app_name: str) -> SparkSession:\n",
    "    \"\"\"\n",
    "    Creates the spark session on the master node.\n",
    "    :param app_name: The name of the app\n",
    "    :return: A SparkSession Object\n",
    "    \"\"\"\n",
    "    conf = SparkConf().setAppName(app_name)\n",
    "    return SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e77824ccb248b6bdbe0a5c6f900d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_quality_checks(df: DataFrame, col: str, schema: Dict[str, Tuple[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Ensure that the dataframe not only has data inside of it, but all the values that belong to a column\n",
    "    reside in that column. The latter is due to the number of unions in this application, so it's possible that\n",
    "    a bad union between two dataframes could lead to values that should belong to id mistakenly end up in\n",
    "    a date column, for example.\n",
    "    :param df: The dataframe to check.\n",
    "    :param col: The column to search for to check for dataframe size.\n",
    "    :param schema: The schema to compare the order of the dataframe's column values.\n",
    "    :return: Nothing.\n",
    "    \"\"\"\n",
    "    assert df.select(col).limit(1).count() > 0\n",
    "    try:\n",
    "        assert df.columns == list(schema)\n",
    "    except AssertionError:\n",
    "        err = \"Dataframe with column structure of {} does not match expected column structure {}.\" \\\n",
    "            .format(df.columns, list(schema))\n",
    "        raise AssertionError(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c0022ed240458f803a83aa4cd960e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_time_and_fact_vals(spark: SparkSession, df: DataFrame, i_type: str,\n",
    "                               time_df: DataFrame or None, fact_df: DataFrame or None) -> (DataFrame, DataFrame):\n",
    "    \"\"\"\n",
    "    Pulling from applicable dataframes, extract the appropriate values for the time and fact dataframes.\n",
    "    Please note, the reason both of these dataframes are filled with one function is to avoid redundancy,\n",
    "    given the simplicity of filling both dataframes.\n",
    "    :param spark: The SparkSession object\n",
    "    :param df: The dataframe to extract values from\n",
    "    :param i_type: The immigration type (asylum, worker, visitor)\n",
    "    :param time_df: The time dataframe to populate\n",
    "    :param fact_df: The fact dataframe to populate.\n",
    "    :return: The time and fact dataframes after getting key values extracted from the passed dataframe.\n",
    "    \"\"\"\n",
    "    time_schema, fact_schema = get_schema('time'), get_schema('fact')\n",
    "    if not time_df:\n",
    "        time_df = make_empty_df(spark, time_schema)\n",
    "    if not fact_df:\n",
    "        fact_df = make_empty_df(spark, fact_schema)\n",
    "    time_fields = ['id', 'immigration_type', 'arrival_year', 'arrival_month', 'arrival_day', 'arrival_weekday',\n",
    "                   'expiry_year', 'expiry_month', 'expiry_day', 'expiry_weekday']\n",
    "    df = df.withColumn('immigration_type', F.lit(i_type))\n",
    "    if i_type == 'asylum':\n",
    "        for_time = _fill_missing_columns(df.selectExpr('id', 'immigration_type', 'year as arrival_year'), time_df)\n",
    "        for_fact = df.selectExpr('id', 'country', 'immigration_type', 'id as time_id')\n",
    "    elif i_type == 'visitor':\n",
    "        for_time = df.select(time_fields)\n",
    "        for_fact = df.selectExpr('id', 'country', 'immigration_type', 'id as time_id')\n",
    "    elif i_type == 'worker':\n",
    "        df = df.withColumn('country', F.lit('Unknown').cast(T.StringType()))\n",
    "        for_time = df.select(time_fields)\n",
    "        for_fact = df.selectExpr('id', 'country', 'immigration_type', 'id as time_id')\n",
    "    else:\n",
    "        raise ValueError(\"{} not a vaid immigration type\".format(i_type))\n",
    "    return time_df.union(for_time.select(list(time_schema))), fact_df.union(for_fact.select(list(fact_schema)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9ab214a93d4383ac95d419b3ed738d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_schema(key: str) -> Dict[str, Tuple[Any, bool]]:\n",
    "    \"\"\"\n",
    "    Function's sole purpose is to hold the schemas of the various dataframes and return the specified one\n",
    "    :param key: The key to the type of schema we want\n",
    "    :return: A schema for a dataframe.\n",
    "    \"\"\"\n",
    "    schemas = {\n",
    "        'asylum': {\n",
    "            'id': (T.LongType(), False,),\n",
    "            'country': (T.StringType(), False,),\n",
    "            'year': (T.LongType(), False,),\n",
    "            'num_arrivals': (T.IntegerType(), True,),\n",
    "            'num_accepted_affirmitavely': (T.IntegerType(), True,),\n",
    "            'num_accepted_defensively': (T.IntegerType(), True,)\n",
    "        },\n",
    "        'country': {\n",
    "            'avg_temperature': (T.IntegerType(), True,),\n",
    "            'avg_temperature_uncertainty': (T.IntegerType(), True,),\n",
    "            'country': (T.StringType(), False,),\n",
    "            'year': (T.IntegerType(), False,),\n",
    "            'month': (T.IntegerType(), False,),\n",
    "            'day': (T.IntegerType(), False,),\n",
    "            'weekday': (T.StringType(), False,),\n",
    "        },\n",
    "        'fact': {\n",
    "            'id': (T.LongType(), False),\n",
    "            'country': (T.StringType(), True,),\n",
    "            'immigration_type': (T.StringType(), False,),\n",
    "            'time_id': (T.LongType(), False,)\n",
    "        },\n",
    "        'time': {\n",
    "            'id': (T.LongType(), False,),\n",
    "            'immigration_type': (T.StringType(), False,),  # will either be visitor, asylum, or worker\n",
    "            'arrival_year': (T.IntegerType(), False,),\n",
    "            'arrival_month': (T.IntegerType(), True,),\n",
    "            'arrival_day': (T.IntegerType(), True,),\n",
    "            'arrival_weekday': (T.IntegerType(), True,),\n",
    "            'expiry_year': (T.IntegerType(), True,),\n",
    "            'expiry_month': (T.IntegerType(), True,),\n",
    "            'expiry_day': (T.IntegerType(), True,),\n",
    "            'expiry_weekday': (T.IntegerType(), True,),\n",
    "        },\n",
    "        'visitor': {\n",
    "            'id': (T.IntegerType(), False,),\n",
    "            'visa_category': (T.StringType(), False,),\n",
    "            'visa_type': (T.StringType(), False,),\n",
    "            'port_of_entry_municipality': (T.StringType(), True,),\n",
    "            'port_of_entry_region': (T.StringType(), True,),\n",
    "            'country': (T.StringType(), False,),\n",
    "            'visiting_state': (T.StringType(), False,),\n",
    "            'arrival_year': (T.IntegerType(), True,),\n",
    "            'arrival_month': (T.IntegerType(), True,),\n",
    "            'arrival_day': (T.IntegerType(), True,),\n",
    "            'arrival_weekday': (T.StringType(), True,),\n",
    "            'expiry_year': (T.IntegerType(), True,),\n",
    "            'expiry_month': (T.IntegerType(), True,),\n",
    "            'expiry_day': (T.IntegerType(), True,),\n",
    "            'expiry_weekday': (T.StringType(), True,)\n",
    "        },\n",
    "        'worker': {\n",
    "            'id': (T.IntegerType(), False,),\n",
    "            'case_status': (T.StringType(), False,),\n",
    "            'visa_type': (T.StringType(), True,),\n",
    "            'employer_name': (T.StringType(), False,),\n",
    "            'employer_city': (T.StringType(), False,),\n",
    "            'employer_state': (T.StringType(), False,),\n",
    "            'worksite_city': (T.StringType(), False,),\n",
    "            'worksite_state': (T.StringType(), False,),\n",
    "            'arrival_year': (T.IntegerType(), False,),\n",
    "            'arrival_month': (T.IntegerType(), True,),\n",
    "            'arrival_day': (T.IntegerType(), True,),\n",
    "            'arrival_weekday': (T.StringType(), True,),\n",
    "            'expiry_year': (T.IntegerType(), True,),\n",
    "            'expiry_month': (T.IntegerType(), True,),\n",
    "            'expiry_day': (T.IntegerType(), True,),\n",
    "            'expiry_weekday': (T.StringType(), True,)\n",
    "        }\n",
    "    }\n",
    "    return schemas[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b6765b3ae64f698b0f1ee9d8d0fdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_empty_df(spark: SparkSession, columns: Dict[str, Tuple[Any, bool]]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create an empty Spark dataframe with a given schema.\n",
    "    :param spark: The SparkSession object\n",
    "    :param columns: A dictionary with the key being the column name, value[0] being the type, value[1] if it's null\n",
    "    :return: An empty dataframe with some schema\n",
    "    \"\"\"\n",
    "    schema = T.StructType([T.StructField(key, columns[key][0], columns[key][1]) for key in columns])\n",
    "    return spark.createDataFrame([], schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b2870a236b40f1b348aec1ee9e5d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_asylum_data(spark: SparkSession, input_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the asylum seeker data to the appropriate schema.\n",
    "    :param spark: the SparkSession object\n",
    "    :param input_path: location of the data\n",
    "    :return: A Spark dataframe\n",
    "    \"\"\"\n",
    "    df = spark.read.csv(input_path + \"refugee_and_migrant_data/*.csv\", header=True) \\\n",
    "        .dropDuplicates() \\\n",
    "        .withColumn('id', F.monotonically_increasing_id())\n",
    "    df = _clean_string_column(df, 'country')\n",
    "    schema = get_schema('asylum')\n",
    "    asylum_df = make_empty_df(spark, schema).union(df.select(list(schema)))\n",
    "    return asylum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd3e2603044450f8d285e046c0be7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_country_climate_data(spark: SparkSession, input_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the asylum seeker data to the appropriate schema.\n",
    "    :param spark: the SparkSession object\n",
    "    :param input_path: location of the data\n",
    "    :return: A Spark dataframe\n",
    "    \"\"\"\n",
    "    cast = ['dt', 'AverageTemperature as avg_temperature',\n",
    "            'AverageTemperatureUncertainty as avg_temperature_uncertainty', 'Country as country']\n",
    "    df = spark.read.csv(input_path + \"climate_data/*.csv\", header=True) \\\n",
    "        .selectExpr(*cast) \\\n",
    "        .dropDuplicates()\n",
    "    df = df.withColumn('date', F.to_date('dt')) \\\n",
    "        .withColumn('year', F.year('date')) \\\n",
    "        .withColumn('month', F.month('date')) \\\n",
    "        .withColumn('day', F.dayofmonth('date')) \\\n",
    "        .withColumn('weekday', F.date_format('date', 'E')) \\\n",
    "        .drop('date', 'dt')\n",
    "    df = _clean_string_column(df, 'country')\n",
    "    schema = get_schema('country')\n",
    "    country_df = make_empty_df(spark, schema).union(df.select(list(schema)))\n",
    "    return country_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998d21e96bbb4cb1aa7b1ab18c31526a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_visitor_data(spark: SparkSession, input_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the asylum seeker data to the appropriate schema.\n",
    "    :param spark: the SparkSession object\n",
    "    :param input_path: location of the data\n",
    "    :return: A Spark dataframe\n",
    "    \"\"\"\n",
    "    cit_and_res = spark.read.json(input_path + \"i94_visitor_data/i94cit_and_i94res.json\", multiLine=True)\n",
    "    port_of_entry = spark.read.json(input_path + \"i94_visitor_data/i94port.json\", multiLine=True)\n",
    "    visa_type = spark.read.json(input_path + \"i94_visitor_data/i94visa.json\", multiLine=True)\n",
    "    to_filter = ['cicid as id', 'i94res', 'i94port', 'arrdate', 'i94visa', 'i94addr as visiting_state',\n",
    "                 'depdate', 'visatype as visa_type']\n",
    "    df = spark.read.parquet(input_path + \"i94_visitor_data/sas_data/*.parquet\").selectExpr(*to_filter).dropDuplicates()\n",
    "    req_lookup = {\n",
    "        'i94res': ['region as country'],\n",
    "        'i94port': ['municipality as port_of_entry_municipality', 'region as port_of_entry_region'],\n",
    "        'i94visa': ['type as visa_category']\n",
    "    }\n",
    "    for name, dtype in df.dtypes:\n",
    "        if dtype == 'double':\n",
    "            df = df.withColumn(name, df[name].cast(T.IntegerType()))\n",
    "        if name in req_lookup:\n",
    "            if name == 'i94res':  # Look up the code to one of the external dataframes\n",
    "                args = (cit_and_res, df, req_lookup[name], name,)\n",
    "            elif name == 'i94port':\n",
    "                args = (port_of_entry, df, req_lookup[name], name,)\n",
    "            elif name == 'i94visa':\n",
    "                args = (visa_type, df, req_lookup[name], name,)\n",
    "            else:\n",
    "                raise ValueError(\"Cannot process {}\".format(name))\n",
    "            df = _code_lookup(*args)\n",
    "        elif name == 'arrdate' or name == 'depdate':  # Number of days since 1/1/1960\n",
    "            prefix = 'arrival' if name == 'arrdate' else 'expiry'\n",
    "            d = \"{}_date\".format(prefix)\n",
    "            df = df.withColumn(d, _convert_to_date(df[name])) \\\n",
    "                .withColumn('{}_year'.format(prefix), F.year(d)) \\\n",
    "                .withColumn('{}_month'.format(prefix), F.month(d)) \\\n",
    "                .withColumn('{}_day'.format(prefix), F.dayofmonth(d)) \\\n",
    "                .withColumn('{}_weekday'.format(prefix), F.date_format(d, 'E')) \\\n",
    "                .drop(d, name)\n",
    "    for column in ['port_of_entry_municipality', 'country', 'visa_category', 'arrival_weekday', 'expiry_weekday']:\n",
    "        df = _clean_string_column(df, column)\n",
    "    schema = get_schema('visitor')\n",
    "    visitor_df = make_empty_df(spark, schema).union(df.select(list(schema)))\n",
    "    return visitor_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd990e7be3744233996aa4cfa1cf1c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_worker_data(spark: SparkSession, input_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the asylum seeker data to the appropriate schema.\n",
    "    :param spark: the SparkSession object\n",
    "    :param input_path: location of the data\n",
    "    :return: A Spark dataframe\n",
    "    \"\"\"\n",
    "    csv, to_filter = \"h1b_kaggle.csv\", [\"CASE_STATUS\", \"EMPLOYER_NAME\", \"YEAR\", \"WORKSITE\"]\n",
    "    df1 = spark.read.csv(input_path + \"legal_immigrant_data/{}\".format(csv), header=True) \\\n",
    "        .selectExpr(*_lower_case_headers(to_filter)) \\\n",
    "        .dropDuplicates() \\\n",
    "        .withColumn(\"visa_class\", F.lit(\"H-1B\"))\n",
    "    df1 = df1.withColumn('split', F.split(df1['worksite'], ',')) \\\n",
    "        .withColumn(\"worksite_city\", F.col('split')[0]) \\\n",
    "        .withColumn(\"worksite_state\", F.col('split')[1]) \\\n",
    "        .drop(\"split\", \"worksite\")\n",
    "    df1 = df1.withColumn('worksite_state', _abbreviate_state(df1.worksite_state))\n",
    "    csv = \"H-1B_Disclosure_Data_FY17.csv\"\n",
    "    to_filter = ['CASE_STATUS', 'VISA_CLASS', 'EMPLOYMENT_START_DATE', 'EMPLOYMENT_END_DATE', 'EMPLOYER_NAME',\n",
    "                 'EMPLOYER_CITY', 'EMPLOYER_STATE', 'WORKSITE_CITY', 'WORKSITE_STATE']\n",
    "    df2 = spark.read.csv(input_path + \"legal_immigrant_data/{}\".format(csv), header=True) \\\n",
    "        .selectExpr(*_lower_case_headers(to_filter)) \\\n",
    "        .dropDuplicates()\n",
    "    states = {\n",
    "        'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY',\n",
    "        'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "        'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'\n",
    "    }\n",
    "    valid_row_allignment = lambda x: (F.length(x) == 2) & (x.isin(states))\n",
    "    df2 = df2.filter(valid_row_allignment(df2.worksite_state)) # Mini data quality check to check row allignment\n",
    "    for d in ['start_date', 'end_date']:\n",
    "        prefix = 'arrival' if d == 'start_date' else 'expiry'\n",
    "        column = 'employment_start_date' if d == 'start_date' else 'employment_end_date'\n",
    "        df2 = df2.withColumn(d, F.to_date(column)) \\\n",
    "            .withColumn('{}_year'.format(prefix), F.year(d)) \\\n",
    "            .withColumn('{}_month'.format(prefix), F.month(d)) \\\n",
    "            .withColumn('{}_day'.format(prefix), F.dayofmonth(d)) \\\n",
    "            .withColumn('{}_weekday'.format(prefix), F.date_format(d, 'E')) \\\n",
    "            .drop(d, column)\n",
    "    new_df = _fill_missing_columns(df1, df2)\n",
    "    new_df = new_df.union(df2).dropDuplicates().withColumn('id', F.monotonically_increasing_id())\n",
    "    new_df = new_df.withColumnRenamed('visa_class', 'visa_type')\n",
    "    for column in ['case_status', 'employer_name', 'worksite_city', 'arrival_weekday', 'expiry_weekday']:\n",
    "        new_df = _clean_string_column(new_df, column)\n",
    "    schema = get_schema('worker')\n",
    "    worker_df = make_empty_df(spark, schema).union(new_df.select(list(schema)))\n",
    "    return worker_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2359b392da694377859c42ef264c8d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def write(df: DataFrame, output_path: str,\n",
    "          partition_by: Optional[List[str]] = None,\n",
    "          mode: Optional[str] = 'overwrite',\n",
    "          format: Optional[str] = 'parquet') -> None:\n",
    "    \"\"\"\n",
    "    Save the dataframe to some storage (s3, HDFS, etc) AFTER a data quality check\n",
    "    :param df: The dataframe to write\n",
    "    :param output_path: where to write it\n",
    "    :param partition_by: how to partition the data (only valid with parquet format)\n",
    "    :param mode: Overwrite, add, etc\n",
    "    :param format: write as a parquet, json, csv, etc\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if partition_by:\n",
    "        df.write.partitionBy(partition_by).format(format).mode(mode).save(output_path)\n",
    "    else:\n",
    "        df.write.format(format).mode(mode).save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6bb14d8f0b4815ad025c4ddac48d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@udf(T.StringType())\n",
    "def _abbreviate_state(state: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to the worker parser that abbreviates a US state\n",
    "    :param state: the name of the state\n",
    "    :return: either the symbol or the original value of state if no valid key is found.\n",
    "    \"\"\"\n",
    "    states = {\n",
    "        'alabama': 'AL', 'alaska': 'AK', 'arizona': 'AZ', 'arkansas': 'AR', 'california': 'CA', 'colorado': 'CO',\n",
    "        'connecticut': 'CT', 'delaware': 'DE', 'florida': 'FL', 'georgia': 'GA', 'hawaii': 'HI', 'idaho': 'ID',\n",
    "        'illinois': 'IL', 'indiana': 'IN', 'iowa': 'IA', 'kansas': 'KS', 'kentucky': 'KY', 'louisiana': 'LA',\n",
    "        'maine': 'ME', 'maryland': 'MD', 'massachusetts': 'MA', 'michigan': 'MI', 'minnesota': 'MN',\n",
    "        'mississippi': 'MS', 'missouri': 'MO', 'montana': 'MT', 'nebraska': 'NE', 'nevada': 'NV', 'new hampshire': 'NH',\n",
    "        'new jersey': 'NJ', 'new mexico': 'NM', 'new york': 'NY', 'north carolina': 'NC', 'north dakota': 'ND',\n",
    "        'ohio': 'OH', 'oklahoma': 'OK', 'oregon': 'OR', 'pennsylvania': 'PA', 'rhode island': 'RI',\n",
    "        'south carolina': 'SC', 'south dakota': 'SD', 'tennessee': 'TN', 'texas': 'TX', 'utah': 'UT', 'vermont': 'VT',\n",
    "        'virginia': 'VA', 'washington': 'WA', 'west virginia': 'WV', 'wisconsin': 'WI', 'wyoming': 'WY',\n",
    "        'district of columbia': 'DC'\n",
    "    }\n",
    "    if state:\n",
    "        s = state[1:].lower()  # each state in the dataframe is prepended with a space thanks to the split function. \n",
    "        return state if s not in states else states[s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70cd618db3a47049f9f6346261c830a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _clean_string_column(df: DataFrame, column_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize the characters in a StringType() column. Lower case them, then replace spaces with '_'\n",
    "    :param df: The dataframe to perform the operation on.\n",
    "    :param column_name: The column to perform the operation on.\n",
    "    :return: The same dataframe with the formatted column.\n",
    "    \"\"\"\n",
    "    return df.withColumn(column_name, F.lower(F.regexp_replace(column_name, ' ', '_')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6b548447ed41b98f3724845de0ea3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _code_lookup(lookup: DataFrame, main_df: DataFrame, to_select: List[str], main_df_code_col: str):\n",
    "    \"\"\"\n",
    "    Helper function for parsing visitor data. The visitor dataframe has numeric codes that need to be looked up by\n",
    "    several other dataframes.\n",
    "    :param lookup: The dataframe that some value corresponds to\n",
    "    :param main_df: The dataframe the value was found in\n",
    "    :param to_select: The columns to select from the lookup dataframe\n",
    "    :param main_df_code_col: The column containing the codes to reference in the lookup dataframe\n",
    "    :return: lookup and main_df joined together at the specific column.\n",
    "    \"\"\"\n",
    "    if 'code' not in to_select:\n",
    "        to_select.append('code')\n",
    "    return lookup.selectExpr(*to_select) \\\n",
    "        .join(main_df, lookup.code == main_df[main_df_code_col]) \\\n",
    "        .drop('code', main_df_code_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48583938f16e44faaa0f92d4168de6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@udf(T.DateType())\n",
    "def _convert_to_date(d: int) -> date:\n",
    "    \"\"\"\n",
    "    Convert dates from a numeric value (number of days since 1/1/1960) to a Date format\n",
    "    :param d: Number of days since 1/1/1960\n",
    "    :return: A date object.\n",
    "    \"\"\"\n",
    "    if not d:\n",
    "        d = 0\n",
    "    return date(1960, 1, 1) + timedelta(days=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07eab05acf344bc2aec32f1d9ec45997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _fill_missing_columns(df_with_missing_cols: DataFrame, df: DataFrame, default: Optional[Any] = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    To perform a union, one needs to have two dataframes with the same columns. This function will fill add on the\n",
    "    columns as needed.\n",
    "    :param df_with_missing_cols: The smaller data frame\n",
    "    :param df: The dataframe with the missing columns the first one needs\n",
    "    :param default: The default value to fill for the missing values.\n",
    "    :return: The smaller dataframe with the added columns.\n",
    "    \"\"\"\n",
    "    if default:\n",
    "        if type(default) is int:\n",
    "            data_type = T.IntegerType\n",
    "        elif type(default) is str:\n",
    "            data_type = T.StringType\n",
    "        elif type(default) is float:\n",
    "            data_type = T.DoubleType\n",
    "        else:\n",
    "            raise ValueError(\"Type {} is not supported.\".format(type(default)))\n",
    "    else:\n",
    "        data_type = T.StringType\n",
    "    cols = set(df_with_missing_cols.columns)\n",
    "    new_df = df_with_missing_cols\n",
    "    for i in df.columns:\n",
    "        if i not in cols:\n",
    "            new_df = new_df.withColumn(i, F.lit(default).cast(data_type()))\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6472ea50fd3942b3af0a23efef294ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _lower_case_headers(columns: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lower the capitalization of the headers in some dataframe.\n",
    "    :param columns: The columns to lower.\n",
    "    :return: A list to pass into a selectExpr statement\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    for name in columns:\n",
    "        if name == 'YEAR':\n",
    "            change_to = 'arrival_year'\n",
    "        else:\n",
    "            change_to = name.lower()\n",
    "        fields.append('{} as {}'.format(name, change_to))\n",
    "    return fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of function definitions. \n",
    "\n",
    "### All cells below this one should be treated as the \"main\" function block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f7878bc22145729e229db5fd9a6021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = create_spark_session('capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb38cc8764249fc8e97aa3b89d3a242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------------+-------+----+-----+---+-------+\n",
      "|   avg_temperature|avg_temperature_uncertainty|country|year|month|day|weekday|\n",
      "+------------------+---------------------------+-------+----+-----+---+-------+\n",
      "|0.4740000000000002|                      3.513|  åland|1756|    4|  1|    Thu|\n",
      "|            15.231|                      2.172|  åland|1777|    8|  1|    Fri|\n",
      "|             8.172|                      2.132|  åland|1779|    5|  1|    Sat|\n",
      "|            -5.925|                       2.34|  åland|1828|    2|  1|    Fri|\n",
      "|5.1480000000000015|         1.4380000000000002|  åland|1845|    5|  1|    Thu|\n",
      "+------------------+---------------------------+-------+----+-----+---+-------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "country_df = parse_country_climate_data(spark, input_path)\n",
    "country_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02545dfa91f440d58ea2098ee9b2d765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----+------------+--------------------------+------------------------+\n",
      "| id|     country|year|num_arrivals|num_accepted_affirmitavely|num_accepted_defensively|\n",
      "+---+------------+----+------------+--------------------------+------------------------+\n",
      "|  0|       benin|2017|        null|                      null|                    null|\n",
      "|  1|     bolivia|2010|        null|                        15|                    null|\n",
      "|  2|       burma|2011|       16972|                       146|                      60|\n",
      "|  3|      mexico|2015|        null|                       657|                     202|\n",
      "|  4|south_africa|2009|        null|                         5|                    null|\n",
      "+---+------------+----+------------+--------------------------+------------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "asylum_df = parse_asylum_data(spark, input_path)\n",
    "asylum_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c8d0c341f54eb59a899f343f5a373f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------+--------------------------+--------------------+--------------+--------------+------------+-------------+-----------+---------------+-----------+------------+----------+--------------+\n",
      "|    id|visa_category|visa_type|port_of_entry_municipality|port_of_entry_region|       country|visiting_state|arrival_year|arrival_month|arrival_day|arrival_weekday|expiry_year|expiry_month|expiry_day|expiry_weekday|\n",
      "+------+-------------+---------+--------------------------+--------------------+--------------+--------------+------------+-------------+-----------+---------------+-----------+------------+----------+--------------+\n",
      "|459727|     pleasure|       WT|                   atlanta|                  GA|united_kingdom|            GA|        2016|            4|          3|            sun|       2016|           4|        11|           mon|\n",
      "|459742|     pleasure|       WT|                   atlanta|                  GA|united_kingdom|            GA|        2016|            4|          3|            sun|       2016|           4|        11|           mon|\n",
      "|459745|     pleasure|       WT|                   atlanta|                  GA|united_kingdom|            GA|        2016|            4|          3|            sun|       2016|           4|        11|           mon|\n",
      "|459883|     pleasure|       WT|                   atlanta|                  GA|united_kingdom|            GE|        2016|            4|          3|            sun|       2016|           4|        11|           mon|\n",
      "|459919|     business|       WB|                   atlanta|                  GA|united_kingdom|            LO|        2016|            4|          3|            sun|       2016|           4|         6|           wed|\n",
      "+------+-------------+---------+--------------------------+--------------------+--------------+--------------+------------+-------------+-----------+---------------+-----------+------------+----------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "visitor_df = parse_visitor_data(spark, input_path)\n",
    "visitor_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6d21e006464cb0a1de5fbb42f96244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+--------------------+-------------+--------------+-------------+--------------+------------+-------------+-----------+---------------+-----------+------------+----------+--------------+\n",
      "| id|        case_status|visa_type|       employer_name|employer_city|employer_state|worksite_city|worksite_state|arrival_year|arrival_month|arrival_day|arrival_weekday|expiry_year|expiry_month|expiry_day|expiry_weekday|\n",
      "+---+-------------------+---------+--------------------+-------------+--------------+-------------+--------------+------------+-------------+-----------+---------------+-----------+------------+----------+--------------+\n",
      "|  0|          certified|     H-1B|  lucus_advisors_llc|         null|          null|     new_york|            NY|        2016|         null|       null|           null|       null|        null|      null|          null|\n",
      "|  1|certified-withdrawn|     H-1B|            easi_llc|         null|          null|        tulsa|            OK|        2016|         null|       null|           null|       null|        null|      null|          null|\n",
      "|  2|          certified|     H-1B|mha_petroleum_con...|         null|          null|       denver|            CO|        2016|         null|       null|           null|       null|        null|      null|          null|\n",
      "|  3|          certified|     H-1B|  vst_consulting_inc|         null|          null|       albany|            NY|        2015|         null|       null|           null|       null|        null|      null|          null|\n",
      "|  4|          certified|     H-1B|        tesaro,_inc.|         null|          null|      waltham|            MA|        2015|         null|       null|           null|       null|        null|      null|          null|\n",
      "+---+-------------------+---------+--------------------+-------------+--------------+-------------+--------------+------------+-------------+-----------+---------------+-----------+------------+----------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "worker_df = parse_worker_data(spark, input_path)\n",
    "worker_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1573ee384b3440d1ad7369ace1829619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------------+-------------+-----------+---------------+-----------+------------+----------+--------------+\n",
      "| id|immigration_type|arrival_year|arrival_month|arrival_day|arrival_weekday|expiry_year|expiry_month|expiry_day|expiry_weekday|\n",
      "+---+----------------+------------+-------------+-----------+---------------+-----------+------------+----------+--------------+\n",
      "|  0|          asylum|        2017|         null|       null|           null|       null|        null|      null|          null|\n",
      "|  1|          asylum|        2010|         null|       null|           null|       null|        null|      null|          null|\n",
      "|  2|          asylum|        2011|         null|       null|           null|       null|        null|      null|          null|\n",
      "|  3|          asylum|        2015|         null|       null|           null|       null|        null|      null|          null|\n",
      "|  4|          asylum|        2009|         null|       null|           null|       null|        null|      null|          null|\n",
      "+---+----------------+------------+-------------+-----------+---------------+-----------+------------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+------------+----------------+-------+\n",
      "| id|     country|immigration_type|time_id|\n",
      "+---+------------+----------------+-------+\n",
      "|  0|       benin|          asylum|      0|\n",
      "|  1|     bolivia|          asylum|      1|\n",
      "|  2|       burma|          asylum|      2|\n",
      "|  3|      mexico|          asylum|      3|\n",
      "|  4|south_africa|          asylum|      4|\n",
      "+---+------------+----------------+-------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "time_df, fact_df = None, None\n",
    "for df, i_type in [(asylum_df, 'asylum',), (visitor_df, 'visitor',), (worker_df, 'worker',)]:\n",
    "    time_df, fact_df = extract_time_and_fact_vals(spark, df, i_type, time_df, fact_df)\n",
    "time_df.show(5)\n",
    "fact_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The last line of the cell is responsible for a serious bottleneck: S3 write speeds. If you would like to see the writing happen with given input and output, uncomment the line.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b945201bcd7f46b19a0959e62fec3876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c790906855f45cb97815392f5bfa7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_check = [\n",
    "    (country_df, 'country', [],), \n",
    "    (asylum_df, 'asylum', ['year'],), \n",
    "    (visitor_df, 'visitor', ['arrival_year', 'arrival_month', 'arrival_day', 'arrival_weekday', 'expiry_year', 'expiry_month', 'expiry_day', 'expiry_weekday'],),\n",
    "    (worker_df, 'worker', ['arrival_year', 'arrival_month', 'arrival_day', 'arrival_weekday', 'expiry_year', 'expiry_month', 'expiry_day', 'expiry_weekday'],),\n",
    "    (time_df, 'time', [],),\n",
    "    (fact_df, 'fact', [],)\n",
    "]\n",
    "write_args = [\n",
    "    ('{}temperatures/'.format(output_path), ['country'],),\n",
    "    ('{}asylum/'.format(output_path), ['country'],),\n",
    "    ('{}visitors/'.format(output_path), ['country', 'visa_category'],),\n",
    "    ('{}workers/'.format(output_path), ['visa_type'],),\n",
    "    ('{}time/'.format(output_path), ['immigration_type', 'arrival_year'],),\n",
    "    ('{}immigration_facts/'.format(output_path), ['immigration_type'],)\n",
    "]\n",
    "for i, (df, key, to_drop) in enumerate(to_check):  # Only write if data quality has been approved.\n",
    "    col = 'id' if df is not country_df else 'country'\n",
    "    data_quality_checks(df, col, get_schema(key))\n",
    "    if to_drop: \n",
    "        df = df.drop(*to_drop)\n",
    "#     write(df, *write_args[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
